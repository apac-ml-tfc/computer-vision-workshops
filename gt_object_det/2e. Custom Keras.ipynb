{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNDER CONSTRUCTION\n",
    "\n",
    "This notebook is still a work in progress and may not work as intended. You have been warned!\n",
    "\n",
    "# Boots 'n' Cats 2e: Modelling with a Custom TF.Keras Algorithm\n",
    "\n",
    "In this notebook we'll try another approach to build our boots 'n' cats detector: a [tf.keras](https://www.tensorflow.org/guide/keras)-based [YOLOv3](https://pjreddie.com/media/files/papers/YOLOv3.pdf) implementation on SageMaker's [TensorFlow container](https://sagemaker.readthedocs.io/en/stable/using_tf.html).\n",
    "\n",
    "SageMaker supports fully custom containers, but also offers pre-optimized environments for the major ML frameworks TensorFlow, PyTorch, and MXNet; which streamline typical workflows.\n",
    "\n",
    "The interface mechanisms (channels, endpoints, etc) work the same as for the built-in algorithms, but now we're authoring a Python package loaded by the framework application inside the base container: So need to understand the interfaces through which our code consumes inputs and exposes results and parameters.\n",
    "\n",
    "**You'll need to** have gone through the first notebook in this series (*Intro and Data Preparation*) to complete this example. The explanatory notes here also reference the equivalent steps of the previous notebook *SageMaker Built-In Algorithm* - so you'll want to read through that if you haven't already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements & About the Implementation\n",
    "\n",
    "For more information about YOLOv3, see the discussion in [Notebook 2c](2c.%20Custom%20MXNet%20YOLO%20Algo.ipynb) where we first introduced it.\n",
    "\n",
    "In this notebook, we'll use a custom adaptation of the open source implementation from https://github.com/qqwweee/keras-yolo3 (MIT Licensed).\n",
    "\n",
    "At a high level, our version was modified to:\n",
    "\n",
    "- Use `tf.keras` (at TensorFlow v1.12) rather than the separate `keras` library (reflecting that Keras ceases to maintain a multi-backend framework as of April 2020. See https://keras.io/ for info)\n",
    "- Train on a `PipeModeDataset` (from [sagemaker-tensorflow](https://github.com/aws/sagemaker-tensorflow-extensions)) rather than a Python generator, to consume training data in Pipe mode like the other implementations in this series.\n",
    "- Remove some features (e.g. transfer learning from published Darknet weights)\n",
    "\n",
    "This modified implementation cuts some features out of the original (e.g. transfer learning from published Darknet weights) and hasn't been benchmarked to validate end-to-end performance:\n",
    "\n",
    "**It isn't intended as a reference implementation for YOLOv3.** Our goal here is to demonstrate the fundamentals of ingesting SageMaker Ground Truth object detection annotations into tf.keras models on SageMaker.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Dependencies and configuration\n",
    "\n",
    "As usual we'll start by loading libraries, defining configuration, and connecting to the AWS SDKs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Built-Ins:\n",
    "import csv\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import imageio\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker.debugger import TensorBoardOutputConfig\n",
    "from sagemaker.tensorflow import TensorFlow as TensorFlowEstimator\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Local Dependencies:\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we re-load configuration from the intro & data processing notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r BUCKET_NAME\n",
    "assert BUCKET_NAME, \"BUCKET_NAME missing from IPython store\"\n",
    "%store -r CHECKPOINTS_PREFIX\n",
    "assert CHECKPOINTS_PREFIX, \"CHECKPOINTS_PREFIX missing from IPython store\"\n",
    "%store -r DATA_PREFIX\n",
    "assert DATA_PREFIX, \"DATA_PREFIX missing from IPython store\"\n",
    "%store -r MODELS_PREFIX\n",
    "assert MODELS_PREFIX, \"MODELS_PREFIX missing from IPython store\"\n",
    "%store -r CLASS_NAMES\n",
    "assert CLASS_NAMES, \"CLASS_NAMES missing from IPython store\"\n",
    "%store -r test_image_folder\n",
    "assert test_image_folder, \"test_image_folder missing from IPython store\"\n",
    "\n",
    "%store -r attribute_names\n",
    "assert attribute_names, \"attribute_names missing from IPython store\"\n",
    "%store -r n_samples_training\n",
    "assert n_samples_training, \"n_samples_training missing from IPython store\"\n",
    "%store -r n_samples_validation\n",
    "assert n_samples_validation, \"n_samples_validation missing from IPython store\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just connect to the AWS SDKs we'll use, and validate the choice of S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "s3 = session.resource(\"s3\")\n",
    "bucket = s3.Bucket(BUCKET_NAME)\n",
    "smclient = session.client(\"sagemaker\")\n",
    "\n",
    "bucket_region = \\\n",
    "    session.client(\"s3\").head_bucket(Bucket=BUCKET_NAME) \\\n",
    "    [\"ResponseMetadata\"][\"HTTPHeaders\"][\"x-amz-bucket-region\"]\n",
    "assert (\n",
    "    bucket_region == region\n",
    "), f\"Your S3 bucket {BUCKET_NAME} and this notebook need to be in the same AWS region.\"\n",
    "\n",
    "# Initialise some empty variables we need to exist:\n",
    "predictor_std = None\n",
    "predictor_hpo = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Review our algorithm details\n",
    "\n",
    "We'll use a custom YOLOv3 implementation on the SageMaker-provided TensorFlow container.\n",
    "\n",
    "As detailed in the [SageMaker Python SDK docs](https://sagemaker.readthedocs.io/en/stable/using_tf.html), our job is to implement a Python file (or bundle of files with a designated entry point) that:\n",
    "\n",
    "* When run as a script, performs model training and saves the resultant model artifacts\n",
    "* Optionally copy definitions for pre- and post-processing functions into the saved model artifact for inference time.\n",
    "\n",
    "In cases like this one where extra dependencies (or newer versions) are required vs the base container, there are three options:\n",
    "\n",
    "* Define a custom container, either inheriting from the SageMaker base container; or creating a compatible fully-custom container\n",
    "* Supplying a `requirements.txt` file at the top level of the source bundle\n",
    "* Performing an inline `pip install` in the code itself, executed when the file is loaded\n",
    "\n",
    "In general the first option is preferred: pre-bundling dependencies into the container is more efficient for (potentially high-value, GPU-accelerated) compute resources than running an install script every time the training job starts or the new inference container instance starts up... But the other options can be useful for quick experimentation.\n",
    "\n",
    "Take some time to look through our implementation at the location below in this repository:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_point=\"keras_main.py\"\n",
    "source_dir=\"src-keras\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with built-in algorithms, the choices we make in implementation will have consequences including for example:\n",
    "\n",
    "* Whether distributed training is supported\n",
    "* Whether GPU-accelerated instances will provide any performance benefits\n",
    "* What data formats are supported for training and inference\n",
    "* How data is loaded into the container at training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set up input data channels\n",
    "\n",
    "From a script complexity perspective, the simplest approach would be to choose `s3_data_type=\"S3Prefix\"` and estimator `input_mode=\"File\"` (the defaults): in which case the matching files in S3 are simply downloaded to the container before the training job starts, and our training job code should load those to an in-memory dataset. This is the approach taken in the [official SageMaker MXNet MNIST example](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/mxnet_mnist).\n",
    "\n",
    "File mode is easy to use, but for jobs with large data sets we can cut container disk size and job startup time requirements by using **Pipe Mode** instead. Instead of downloading the full dataset to your container before starting work, Pipe Mode sets up a **stream** - which looks like a file on disk but only supports sequential access. Because the stream can't seek back to the start, SageMaker creates a new stream for each epoch - for example `/opt/ml/input/train/train_0` for the first.\n",
    "\n",
    "We'd still like SageMaker to shuffle our data, so how do we handle a use case like this where we need *both* the image file and its corresponding annotations to be useful?\n",
    "\n",
    "One solution could be to provide two separate channels for the images and the annotations (like the SageMaker built-in Image Classification algorithm when training with [\"Image Format\"](https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html)).\n",
    "\n",
    "A more feature-rich approach is the **AugmentedManifestFile** data type we used in the previous example. With this method, SageMaker steps through the manifest file and produces *multiple records in the stream for each row*: One for each (potentially filtered) attribute.\n",
    "\n",
    "Attribute names ending `-ref` (like our `source-ref`) are resolved to the raw contents of the S3 file URI they point to. Other attributes are passed through as plain text (i.e. JSON)... So in our case, the file stream in the container receives alternating records of image data and JSON annotations.\n",
    "\n",
    "As you might imagine (and will see in the training script file), using Pipe Mode and AugmentedManifestFile means we have to translate this stream into the (probably different) format our models expect. As we're using TensorFlow, we'll use the [PipeModeDataset](https://github.com/aws/sagemaker-tensorflow-extensions#sagemaker-pipe-mode) class provided by the [sagemaker-tensorflow](https://github.com/aws/sagemaker-tensorflow-extensions) package (already installed on the SageMaker TensorFlow container images).\n",
    "\n",
    "Here we'll use the **same AugmentedManifestFile channel configuration as the built-in algorithm** from the previous notebook; and stream batches of data in to PipeModeDatasets to train one after the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = sagemaker.session.s3_input(\n",
    "    f\"s3://{BUCKET_NAME}/{DATA_PREFIX}/train.manifest\",\n",
    "    distribution=\"FullyReplicated\",  # In case we want to try distributed training\n",
    "    content_type=\"application/x-recordio\",\n",
    "    s3_data_type=\"AugmentedManifestFile\",\n",
    "    record_wrapping=\"RecordIO\",\n",
    "    attribute_names=attribute_names,  # In case the manifest contains other junk to ignore (it does!)\n",
    "    shuffle_config=sagemaker.session.ShuffleConfig(seed=1337)\n",
    ")\n",
    "                                        \n",
    "validation_channel = sagemaker.session.s3_input(\n",
    "    f\"s3://{BUCKET_NAME}/{DATA_PREFIX}/validation.manifest\",\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"application/x-recordio\",\n",
    "    record_wrapping=\"RecordIO\",\n",
    "    s3_data_type=\"AugmentedManifestFile\",\n",
    "    attribute_names=attribute_names\n",
    ")\n",
    "                                        \n",
    "pretrain_channel = sagemaker.session.s3_input(\n",
    "    f\"s3://{BUCKET_NAME}/{DATA_PREFIX}/darknet\",\n",
    "    distribution=\"FullyReplicated\",\n",
    "    input_mode=\"File\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original https://github.com/qqwweee/keras-yolo3 implementation also supports **transfer learning** by loading the pre-trained [Darknet](https://pjreddie.com/darknet/) weights published on the [YOLOv3 website](https://pjreddie.com/darknet/yolo/) into the Keras model.\n",
    "\n",
    "...But unfortunately `tf.keras.Model.load_weights` at TensorFlow v1.12 doesn't support the `skip_mismatch` parameter available in standalone Keras.\n",
    "\n",
    "We've kept a partial implementation for pre-training in the code (by passing these raw Darknet cfg+weights through as a \"darknet\" channel), but the structure mismatch currently causes training to error so the data preparation in this notebook is commented out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Fix loading weights with mismatch to use Darknet pre-training\n",
    "\n",
    "# !wget -O data/darknet/yolov3-416.weights https://pjreddie.com/media/files/yolov3.weights\n",
    "# !wget -O data/darknet/yolov3-416.cfg https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\n",
    "# bucket.upload_file(\"data/darknet/yolov3-416.cfg\", \"data/darknet/yolov3-416.cfg\")\n",
    "# bucket.upload_file(\"data/darknet/yolov3-416.weights\", \"data/darknet/yolov3-416.weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure the algorithm\n",
    "\n",
    "The core configuration process for the custom algorithm is the same as for the built-in algorithm (such as instance type/count, data source, execution role, and so on)... and we've written this script to accept training and inference data in the same format as the built-in.\n",
    "\n",
    "There are some differences too though, and some of the most significant ones are as follows:\n",
    "\n",
    "**Need to define metrics:** Metrics in SageMaker (like `validation:mAP` which we see in the SageMaker console and used to tune the built-in algorithm) are captured from the console output of the algorithm via a [regular expression](https://www.regular-expressions.info/). For the built-in algorithms, SageMaker already knows what metrics are defined... But for a custom algorithm (since we could be `print()`ing whatever we like), we need to supply the definitions.\n",
    "\n",
    "**No explicit \"training_image\"...:** As discussed in the last notebook, we only needed to use the generic `Estimator` SDK and supply a Docker image URI for the built-in SSD algorithm because there wasn't a specific SDK class for it. MXNet [has one](https://sagemaker.readthedocs.io/en/stable/using_mxnet.html) though, so the image URI is implicit. You can find more details about the (actually separate) SageMaker MXNet [training](https://github.com/aws/sagemaker-mxnet-container) and [inference](https://github.com/aws/sagemaker-mxnet-serving-container) containers in GitHub, as well as the shared [cross-framework base](https://github.com/aws/sagemaker-containers) they inherit from.\n",
    "\n",
    "**...But need to specify framework versions:** Because multiple incompatible versions of Python (2 vs 3), or the ML framework (e.g. TensorFlow 1 vs 2) may be supported at a given time.\n",
    "\n",
    "**Custom hyperparameters:** As with metrics, it's entirely up to us what hyperparameters we want to support and how: So any hyperparameters are fully custom to our code in the [src](src) folder. There's also a little inconsistency between frameworks and built-in algorithms about whether hyperparameters are passed in as a `hyperparameters` argument in the constructor or with a separate `set_hyperparameters()` function, but otherwise the process is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    # You should see these metrics appear in the SageMaker console on the details page for the\n",
    "    # training job when it starts (and see them in the log stream during `.fit()`):\n",
    "    { \"Name\": \"train:YOLOLoss\", \"Regex\": r\" loss: ([\\d\\.]+)\"},\n",
    "    { \"Name\": \"validation:YOLOLoss\", \"Regex\": r\" val_loss: ([\\d\\.]+)\"},\n",
    "]\n",
    "\n",
    "# Review which of these parameters correspond to the built-in notebook, and which are new:\n",
    "estimator = TensorFlowEstimator(\n",
    "    role=role,\n",
    "    entry_point=entry_point,\n",
    "    source_dir=source_dir,\n",
    "    framework_version=\"1.12\",\n",
    "    py_version=\"py3\",\n",
    "    input_mode=\"Pipe\",\n",
    "    train_instance_count=1,  # Note: Our implementation doesn't actually support multi-instance yet\n",
    "    train_instance_type=\"ml.p3.2xlarge\",\n",
    "    train_volume_size=50,\n",
    "    train_max_run=60*60,\n",
    "    # TODO: Support checkpointing in algo so that spot interrupt doesn't restart training\n",
    "    train_use_spot_instances=True,\n",
    "    train_max_wait=60*60,\n",
    "    metric_definitions=metric_definitions,\n",
    "    base_job_name=\"bootsncats-keras\",\n",
    "    output_path=f\"s3://{BUCKET_NAME}/{MODELS_PREFIX}\",\n",
    "    checkpoint_s3_uri=f\"s3://{BUCKET_NAME}/{CHECKPOINTS_PREFIX}\",\n",
    "    # There's more documentation of the hyperparameters in the script itself:\n",
    "    hyperparameters={\n",
    "        \"checkpoint-interval\": 1,\n",
    "        \"epochs\": 60,\n",
    "        \"epochs-stabilize\": 30,\n",
    "        \"learning-rate\": 0.0001,\n",
    "        \"num-classes\": 2,\n",
    "        \"num-samples-train\": n_samples_training,  # Needed for Pipe Mode epochs integration\n",
    "        \"num-samples-validation\": n_samples_validation,  # Needed for Pipe Mode\n",
    "        \"batch-size\": 2,  # Must be an exact divisor of both num-samples counts for Keras\n",
    "        \"data-shape\": 416,\n",
    "        \"random-seed\": 1337,\n",
    "        #\"log-level\": \"DEBUG\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train the model\n",
    "\n",
    "As with the built-in algorithms, we have the choice between fitting our model with the given set of hyperparameters or performing automatic hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WITH_HPO = # TODO: True first, then False?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if (not WITH_HPO):\n",
    "    estimator.fit(\n",
    "        {\n",
    "            #\"darknet\": pretrain_channel,\n",
    "            \"train\": train_channel,\n",
    "            \"validation\": validation_channel,\n",
    "        },\n",
    "        logs=True,\n",
    "    )\n",
    "else:\n",
    "    hyperparameter_ranges = {\n",
    "        \"learning-rate\": sagemaker.tuner.ContinuousParameter(0.00001, 0.01),\n",
    "    }\n",
    "\n",
    "    tuner = sagemaker.tuner.HyperparameterTuner(\n",
    "        estimator,\n",
    "        \"validation:YOLOLoss\",  # Name of the objective metric to optimize\n",
    "        objective_type=\"Minimize\",  # loss low = good\n",
    "        metric_definitions=metric_definitions,\n",
    "        hyperparameter_ranges=hyperparameter_ranges,\n",
    "        base_tuning_job_name=\"bootsncats-keras-hpo\",\n",
    "        # `max_jobs` obviously has cost implications, but the optimization can always be terminated:\n",
    "        max_jobs=8,\n",
    "        max_parallel_jobs=2  # Keep sensible for the configured max_jobs...\n",
    "    )\n",
    "    \n",
    "    tuner.fit(\n",
    "        {\n",
    "            #\"darknet\": pretrain_channel,\n",
    "            \"train\": train_channel,\n",
    "            \"validation\": validation_channel\n",
    "        },\n",
    "        include_cls_metadata=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, if we ever lose notebook state e.g. due to a kernel restart or crash, we can `attach()` our estimator/tuner to a previous training/tuning job as follows: (No need to re-train - the results are all stored!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Examples to attach to a previous training run:\n",
    "#estimator.attach(\"bootsncats-keras-2020-04-24-06-37-41-027\")\n",
    "#tuner.attach(\"bootsncats-keras-hpo-191209-1637\")\n",
    "#WITH_HPO=?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: While the model(s) are training\n",
    "\n",
    "Remember to go back to the previous notebooks if you still have steps unfinished...\n",
    "\n",
    "Otherwise, take some time to read through and understand the code in the src folder! This implementation is made unusually complex, because of the nature of object detection data (image + bounding boxes) and our decision to support the same I/O as the built-in algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Deploy the model\n",
    "\n",
    "Deploying our trained custom model is as simple as for the built-in algorithm, with just one extra step:\n",
    "\n",
    "We need to warn the `predictor` our request body will be an image, so it passes this information along to our container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if (WITH_HPO):\n",
    "    if (predictor_hpo):\n",
    "        try:\n",
    "            predictor_hpo.delete_endpoint()\n",
    "            print(\"Deleted previous HPO endpoint\")\n",
    "        except:\n",
    "            print(\"Couldn't delete previous HPO endpoint\")\n",
    "    print(\"Deploying HPO model...\")\n",
    "    predictor_hpo = tuner.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=\"ml.m5.large\",\n",
    "        # wait=False,\n",
    "    )\n",
    "    predictor_hpo.content_type = \"application/x-image\"\n",
    "else:\n",
    "    if (predictor_std):\n",
    "        try:\n",
    "            predictor_std.delete_endpoint()\n",
    "            print(\"Deleted previous non-HPO endpoint\")\n",
    "        except:\n",
    "            print(\"Couldn't delete previous non-HPO endpoint\")\n",
    "    print(\"Deploying standard (non-HPO) model...\")\n",
    "    predictor_std = estimator.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=\"ml.m5.large\",\n",
    "        endpoint_type=\"tensorflow-serving\",\n",
    "        # wait=False,\n",
    "    )\n",
    "    predictor_std.content_type = \"application/x-image\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Run inference on test images\n",
    "\n",
    "We've set our custom algorithm up like the built-in, to return *all* detections and let the caller filter by whatever confidence threshold seems to perform best.\n",
    "\n",
    "Training this algorithm can be even trickier than SSD though: are you able to get good results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change this if you want something different:\n",
    "predictor = predictor_hpo if WITH_HPO else predictor_std\n",
    "\n",
    "# This time confidence is 0-1, not 0-100:\n",
    "confidence_threshold = 0.2  # TODO: 0.2 is a good starting point, but explore options!\n",
    "\n",
    "for test_image in os.listdir(test_image_folder):\n",
    "    test_image_path = f\"{test_image_folder}/{test_image}\"\n",
    "    with open(test_image_path, \"rb\") as f:\n",
    "        payload = bytearray(f.read())\n",
    "\n",
    "    client = boto3.client(\"sagemaker-runtime\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=predictor.endpoint,\n",
    "        ContentType='application/x-image',\n",
    "        Body=payload\n",
    "    )\n",
    "\n",
    "    result = response[\"Body\"].read()\n",
    "    result = json.loads(result)\n",
    "    boxes = np.array(result[\"predictions\"])[:50]\n",
    "\n",
    "    display(HTML(f\"<h4>{test_image}</h4>\"))\n",
    "    util.visualize_detection(\n",
    "        test_image_path,\n",
    "        boxes,\n",
    "        CLASS_NAMES,\n",
    "        thresh=confidence_threshold,\n",
    "        # This algo returns already-normalized bounding box coordinates:\n",
    "        normalized_coords=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Although training instances are ephemeral, the resources we allocated for real-time endpoints need to be cleaned up to avoid ongoing charges.\n",
    "\n",
    "The code below will delete the *most recently deployed* endpoint for the HPO and non-HPO configurations, but note that if you deployed either more than once, you might end up with extra endpoints.\n",
    "\n",
    "To be safe, it's best to still check through the SageMaker console for any left-over resources when cleaning up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (predictor_hpo):\n",
    "    print(\"Deleting HPO-optimized predictor endpoint\")\n",
    "    predictor_hpo.delete_endpoint()\n",
    "if (predictor_std):\n",
    "    print(\"Deleting standard (non-HPO) predictor endpoint\")\n",
    "    predictor_std.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review TODO\n",
    "\n",
    "**TODO: Review**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
