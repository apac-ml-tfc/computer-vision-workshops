{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with AWS: A Demo with Boots and Cats\n",
    "\n",
    "This series of notebooks demonstrates tackling a sample computer vision problem on AWS - building a two-class object detector for [boots and cats](https://www.youtube.com/watch?v=Nni0rTLg5B8).\n",
    "\n",
    "**This notebook** walks through using the [SageMaker Ground Truth](https://aws.amazon.com/sagemaker/groundtruth/) tool to annotate training and validation data sets.\n",
    "\n",
    "**Follow-on** notebooks show how to train a range of models from the created dataset, including:\n",
    "\n",
    "* [Amazon Rekognition](https://aws.amazon.com/rekognition/)'s new [custom labels](https://aws.amazon.com/rekognition/custom-labels-features/) functionality, announced at Re:Invent 2019\n",
    "* SageMaker's [built-in object detection algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html)\n",
    "* Custom algorithms implemented on SageMaker's **framework containers**, such as MXNet and TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boots 'n' Cats 1: Introduction and Data Preparation\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "We use the [**Open Images Dataset v4**](https://storage.googleapis.com/openimages/web/download_v4.html) as a convenient source of pre-curated images. The Open Images Dataset V4 is created by Google Inc. We have not modified the images or the accompanying annotations. You can obtain the images and the annotations [here](https://storage.googleapis.com/openimages/web/download_v4.html). The annotations are licensed by Google Inc. under CC BY 4.0 license. The images are listed as having a CC BY 2.0 license. The following paper describes Open Images V4 in depth: from the data collection and annotation to detailed statistics about the data and evaluation of models trained on it.\n",
    "\n",
    "A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont-Tuset, S. Kamali, S. Popov, M. Malloci, T. Duerig, and V. Ferrari. The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale. arXiv:1811.00982, 2018. ([link to PDF](https://arxiv.org/abs/1811.00982))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "This notebook is designed to be run in Amazon SageMaker. To complete this workshop (and understand what's going on), you'll need:\n",
    "\n",
    "* Basic familiarity with Python, [AWS S3](https://docs.aws.amazon.com/s3/index.html), [Amazon Sagemaker](https://aws.amazon.com/sagemaker/), and the [AWS Command Line Interface (CLI)](https://aws.amazon.com/cli/).\n",
    "* To run in **a region where [Rekognition Custom Labels](https://aws.amazon.com/rekognition/custom-labels-features/) is available** - Currently US East (N.Virginia), US East (Ohio), US West (Oregon), and EU (Ireland)) - if you plan to explore this feature.\n",
    "* Sufficient [SageMaker quota limits](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html#limits_sagemaker) set on your account to run GPU-accelerated training jobs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost and runtime\n",
    "\n",
    "Depending on your configuration, this demo may consume resources outside of the free tier - but should not generally be expensive, as we'll be training on a small number of images. You might wish to review the following for your region:\n",
    "\n",
    "* [Amazon SageMaker pricing](https://aws.amazon.com/sagemaker/pricing/)\n",
    "* [SageMaker Ground Truth pricing](https://aws.amazon.com/sagemaker/groundtruth/pricing/)\n",
    "* [Amazon Rekognition pricing](https://aws.amazon.com/rekognition/pricing/)\n",
    "\n",
    "The standard `ml.t2.medium` instance should be sufficient to run the notebooks.\n",
    "\n",
    "We will use GPU-accelerated instance types for training and hyperparameter optimization, and use spot instances where appropriate to optimize these costs.\n",
    "\n",
    "As noted in the step-by-step guidance, you should take particular care to delete any created SageMaker real-time prediction endpoints when finishing the demo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Dependencies and configuration\n",
    "\n",
    "As usual we'll start by loading libraries, defining configuration, and connecting to the AWS SDKs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# Built-Ins:\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import sagemaker\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Local Dependencies:\n",
    "# We define some functions in the `util` folder to simplify data preparation and\n",
    "# visualization for the notebook.\n",
    "%aimport util\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we configure the name and layout of your bucket, and the annotation job to set up.\n",
    "\n",
    "**If you're following this demo in a group:** you can pool your annotations for better accuracy without spending hours annotating:\n",
    "\n",
    "* Have each group member set a different `BATCH_OFFSET` (from 0, in increments of `N_EXAMPLES_PER_CLASS`), and you'll be allocated different images to annotate.\n",
    "* Later, you can *import* the other members' output manifest files to your own S3 data set.\n",
    "\n",
    "**If not: don't worry** - we already provide a 100-image set in this repository to augment your annotations!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overall S3 bucket layout:\n",
    "BUCKET_NAME = sagemaker.Session().default_bucket()  # (Or an existing bucket's name, if you prefer)\n",
    "%store BUCKET_NAME\n",
    "DATA_PREFIX = \"data\"  # The folder in the bucket (and locally) where we will store data\n",
    "%store DATA_PREFIX\n",
    "MODELS_PREFIX = \"models\"  # The folder in the bucket where we will store models\n",
    "%store MODELS_PREFIX\n",
    "CHECKPOINTS_PREFIX = \"models/checkpoints\"  # Model checkpoints can go in a subfolder of models\n",
    "%store CHECKPOINTS_PREFIX\n",
    "\n",
    "## Annotation job:\n",
    "CLASS_NAMES = [\"Boot\", \"Cat\"]\n",
    "%store CLASS_NAMES\n",
    "N_EXAMPLES_PER_CLASS = 20\n",
    "BATCH_OFFSET = 0\n",
    "BATCH_NAME = \"my-annotations\"\n",
    "\n",
    "# Note that some paths are reserved, restricting your choice of BATCH_NAME:\n",
    "data_raw_prefix = f\"{DATA_PREFIX}/raw\"\n",
    "data_augment_prefix = f\"{DATA_PREFIX}/augmentation\"\n",
    "data_batch_prefix = f\"{DATA_PREFIX}/{BATCH_NAME}\"\n",
    "test_image_folder = f\"{DATA_PREFIX}/test\"\n",
    "%store test_image_folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just connect to the AWS SDKs we'll use, and validate the choice of S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "s3 = session.resource(\"s3\")\n",
    "bucket = s3.Bucket(BUCKET_NAME)\n",
    "smclient = session.client(\"sagemaker\")\n",
    "\n",
    "bucket_region = \\\n",
    "    session.client(\"s3\").head_bucket(Bucket=BUCKET_NAME) \\\n",
    "    [\"ResponseMetadata\"][\"HTTPHeaders\"][\"x-amz-bucket-region\"]\n",
    "assert (\n",
    "    bucket_region == region\n",
    "), f\"Your S3 bucket {BUCKET_NAME} and this notebook need to be in the same AWS region.\"\n",
    "\n",
    "if region not in (\"eu-west-1\", \"us-east-1\", \"us-east-2\", \"us-west-2\"):\n",
    "    warnings.warn(\n",
    "        f\"**WARNING:**\\nCurrent region {region} is not yet supported by Rekognition Custom Labels!\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set the goalposts with some unlabelled target data\n",
    "\n",
    "Let's start out by collecting a handful of images from around the web to illustrate what we'd like to detect.\n",
    "\n",
    "These images are not licensed and the links may break for different regions / times in future: Feel free to add your own or replace with any other images of boots and cats! Model evaluations in following notebooks will loop through all images in the `test_image_folder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(test_image_folder, exist_ok=True)\n",
    "!wget -O $test_image_folder/tabby.jpg https://images.fineartamerica.com/images-medium-large-5/1990s-ginger-and-white-tabby-cat-animal-images.jpg\n",
    "!wget -O $test_image_folder/beatbox.jpg https://midnightmusic.com.au/wp-content/uploads/2014/08/How-to-beatbox-5001.png\n",
    "!wget -O $test_image_folder/ampersand.jpg https://i.ytimg.com/vi/DsC5hNYpP9Y/maxresdefault.jpg\n",
    "!wget -O $test_image_folder/boots.jpg https://d28m5bx785ox17.cloudfront.net/v1/img/w4r1gr5IKcC9tTcJG_vsJVbyjZ_SVKuFf3YBxtrGdFs=/d/l\n",
    "!wget -O $test_image_folder/cats.jpg https://www.dw.com/image/42582511_401.jpg\n",
    "\n",
    "for test_image in os.listdir(test_image_folder):\n",
    "    display(HTML(f\"<h4>{test_image}</h4>\"))\n",
    "    util.visualize_detection(f\"{test_image_folder}/{test_image}\", [], [])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Map our class names to OpenImages class IDs\n",
    "\n",
    "OpenImages defines a hierarchy of object types (e.g. \"swan\" is a subtype of \"bird\"), and references each with a class ID instead of the human-readable name.\n",
    "\n",
    "Since we want to find images containing boots and cats, our first job is to figure what OpenImages class IDs they correspond to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download and process the Open Images metadata:\n",
    "annotations, class_descriptions, ontology = util.openimages.download_openimages_metadata(data_raw_prefix)\n",
    "\n",
    "# Map our configured CLASS_NAMES to sets of Open Images class IDs:\n",
    "class_id_sets = util.openimages.class_names_to_openimages_ids(CLASS_NAMES, class_descriptions, ontology)\n",
    "print(class_id_sets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Find suitable example images\n",
    "\n",
    "Now we've looked up the full range of applicable label IDs, we can use the OpenImages annotations to extract which image IDs will be interesting for us to train on (i.e. they contain boots and/or cats).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip these images with known bad quality content:\n",
    "SKIP_IMAGES = { \"251d4c429f6f9c39\", \"065ad49f98157c8d\" }\n",
    "\n",
    "image_ids = util.openimages.list_images_containing(\n",
    "    class_id_sets,\n",
    "    annotations,\n",
    "    N_EXAMPLES_PER_CLASS,\n",
    "    SKIP_IMAGES,\n",
    "    BATCH_OFFSET\n",
    ")\n",
    "print(f\"Found {len(image_ids)} images\")\n",
    "print(image_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Upload images and manifest file to S3\n",
    "\n",
    "We need our training image data in an accessible S3 bucket, and a [manifest](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-data-input.html) file defining for SageMaker Ground Truth (and later our model) what images are in the data set and where to find them.\n",
    "\n",
    "In the following cell, we:\n",
    "\n",
    "* Copy each identified image directly from the OpenImages repository to our bucket\n",
    "* Build up a local manifest file listing all the images\n",
    "* Upload the manifest file to the bucket\n",
    "\n",
    "This process should only take a few seconds with small data sets like we're dealing with here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{data_batch_prefix}/manifests\", exist_ok=True)\n",
    "input_manifest_loc = f\"{data_batch_prefix}/manifests/input.manifest\"\n",
    "\n",
    "with open(input_manifest_loc, \"w\") as f:\n",
    "    print(\"Copying images\", end=\"\")\n",
    "    # TODO: Delete existing folder contents?\n",
    "    for image_id in image_ids:\n",
    "        print(\".\", end=\"\")\n",
    "        dest_key = f\"{data_batch_prefix}/images/{image_id}.jpg\"\n",
    "        bucket.copy(\n",
    "            {\n",
    "                \"Bucket\": \"open-images-dataset\",\n",
    "                \"Key\": f\"test/{image_id}.jpg\"\n",
    "            },\n",
    "            dest_key\n",
    "        )\n",
    "        f.write(json.dumps({ \"source-ref\": f\"s3://{BUCKET_NAME}/{dest_key}\" }) + \"\\n\")\n",
    "    print(\"\")\n",
    "    print(f\"Images copied to s3://{BUCKET_NAME}/{data_batch_prefix}/images/\")\n",
    "\n",
    "bucket.upload_file(input_manifest_loc, input_manifest_loc)\n",
    "print(f\"Manifest uploaded to s3://{BUCKET_NAME}/{input_manifest_loc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Set up the SageMaker Ground Truth labelling job\n",
    "\n",
    "Now that our images and a manifest file listing them are ready in S3, we'll set up the Ground Truth labelling job **in the [AWS console](https://console.aws.amazon.com)**.\n",
    "\n",
    "Under *Services* go to *Amazon SageMaker*, and select *Ground Truth > Labeling Jobs* from the side-bar menu on the left.\n",
    "\n",
    "**Note:** These steps assume you've either never used SageMaker Ground Truth before, or have already set up a Private Workforce that will be suitable for this task. If you have one or more private workforces configured already, but none of them are appropriate for this task, you'll need to go to *Ground Truth > Labeling workforces* **first** to create a new one.\n",
    "\n",
    "### Job Details\n",
    "\n",
    "Click the **Create labeling job** button, and you'll be asked to specify job details as follows:\n",
    "\n",
    "* **Job name:** Choose a name to identify this labelling job, e.g. `boots-and-cats-batch-0`\n",
    "* **Input data location:** The path to the input manifest file in S3 (see output above)\n",
    "* **Output data location:** Set this just to the parent folder of the input manifests (e.g. *s3://gt-object-detect-thewsey-us-east-1/data/my-annotations*)\n",
    "* **IAM role:** If you're not sure whether your existing roles have the sufficient permissions for Ground Truth, select the options to create a new role\n",
    "* **Task type:** Image > Bounding box\n",
    "\n",
    "<img src=\"BlogImages/JobDetailsIntro.png\"/>\n",
    "\n",
    "All other settings can be left as default. Record your choices for the label name and output data location below, because we'll need these later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_groundtruth_job_name =  # TODO: e.g. \"boots-and-cats-batch-0\"?\n",
    "my_groundtruth_labels = my_groundtruth_job_name  # Shouldn't need to change, if you left this as default\n",
    "my_groundtruth_output = f\"s3://{BUCKET_NAME}/data/my-annotations\"  # TODO: **No trailing slash!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workers\n",
    "\n",
    "On the next screen, we'll configure **who** will annotate our data: Ground Truth allows you to define your own in-house *Private Workforces*; use *Vendor Managed Workforces* for specialist tasks; or use the public workforce provided by *Amazon Mechanical Turk*.\n",
    "\n",
    "Select **Private** worker type, and you'll be prompted either to select from your existing private workforces, or create a new one if none exist.\n",
    "\n",
    "To create a new private workforce if you need, simply follow the UI workflow with default settings. It doesn't matter what you call the workforce, and you can create a new Cognito User Group to define the workforce. **Add yourself** to the user pool by adding your email address: You should receive a confirmation email shortly with a temporary password and a link to access the annotation portal.\n",
    "\n",
    "Automatic data labeling is applicable only for data sets over 1000 samples, so leave this turned **off** for now.\n",
    "\n",
    "<img src=\"BlogImages/SelectPrivateWorkforce.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling Tool\n",
    "\n",
    "Since you'll be labelling the data yourself, a brief description of the task should be fine in this case. When using real workforces, it's important to be really clear in this section about the task requirements and best practices - to ensure consistency of annotations between human workers.\n",
    "\n",
    "For example: In the common case where we see a *pair* of boots from the side and one is almost entirely obscured, how should the image be annotated? Should *model* cats count, or only real ones?\n",
    "\n",
    "The most important configuration here is to set the *options* to be the same as our `CLASS_NAMES` and in the same order: **Boot, Cat**\n",
    "\n",
    "<img src=\"BlogImages/LabellingToolSetup.png\"/>\n",
    "\n",
    "Take some time to explore the other options for configuring the annotation tool; and when you're ready click \"Create\" to launch the labeling job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Label those images!\n",
    "\n",
    "Follow the link you received in your workforce invitation email to the workforce's **labelling portal**, and log in with the default password given in the email (which you'll be asked to change).\n",
    "\n",
    "If you lose the portal link, you can always retrieve it through the *Ground Truth > Labeling Workforces* menu in the SageMaker console: Near the top of the summary of private workforces.\n",
    "\n",
    "New jobs can sometimes take a minute or two to appear for workers, but you should soon see a screen like the below. Select the job and click \"Start working\" to enter the labelling tool.\n",
    "\n",
    "<img src=\"BlogImages/LabellingJobsReady.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can check on the progress of labelling jobs through the APIs as well as in the AWS console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smclient.describe_labeling_job(LabelingJobName=my_groundtruth_job_name)[\"LabelingJobStatus\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label all the images in the tool by selecting the class and drawing boxes around the objects, and when done you will be brought back to the (now empty) jobs list screen above.\n",
    "\n",
    "It may take a few seconds after completing for the job status to update in the AWS console.\n",
    "\n",
    "When the job shows as complete, run the below code to **download your results:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker Ground Truth will save your actual output manifest to here:\n",
    "smgt_output_manifest_uri = f\"{my_groundtruth_output}/{my_groundtruth_job_name}/manifests/output/output.manifest\"\n",
    "smgt_output_bucket, my_smgt_output_key = util.smgt.s3_uri_to_bucket_and_key(smgt_output_manifest_uri)\n",
    "\n",
    "# That path's bonkers convoluted, so let's have our local copy somewhere a little more concise:\n",
    "my_smgt_output_path = f\"{data_batch_prefix}/manifests/output.manifest\"\n",
    "\n",
    "print(f\"Downlading output manifest:\\n{smgt_output_manifest_uri}\")\n",
    "bucket.download_file(my_smgt_output_key, my_smgt_output_path)\n",
    "print(f\"\\nGot: {my_smgt_output_path}\")\n",
    "\n",
    "print(f\"\\nContents:\")\n",
    "with open(my_smgt_output_path, \"r\") as f:\n",
    "    print(f.readline()[:-1]) # (Strip trailing newline)\n",
    "print(\"...etc.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Import an additional pre-labelled dataset\n",
    "\n",
    "This repository contains an example output manifest (100 images) which we can use to augment our data set and improve our model's accuracy (or in case you couldn't finish your labelling job!).\n",
    "\n",
    "Of course, somebody else's manifest will reference files in their bucket - that we probably don't have access to... So here we **import** these refs (openimages JPEGs) to our own bucket, and create a new manifest file with the updated links.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename to output the translated manifest to (we'll need this later):\n",
    "augment_manifest_path = f\"{data_augment_prefix}/manifests/output/output.updated.manifest\"\n",
    "\n",
    "# Function which, given a ref, will upload the file to our bucket and return the new ref:\n",
    "import_fn = util.smgt.ManifestRefImporter(\n",
    "    source=lambda s: s.rpartition(\"/\")[2],\n",
    "    target=f\"s3://{BUCKET_NAME}/{data_augment_prefix}/images/\",\n",
    "    repository=\"s3://open-images-dataset/test/\",\n",
    "    session=session,\n",
    ")\n",
    "\n",
    "# Translate the manifest:\n",
    "print(\"Importing augmentation manifest refs...\")\n",
    "util.smgt.translate_manifest_refs(\n",
    "    source_manifest=f\"{data_augment_prefix}/manifests/output/output.manifest\",\n",
    "    target_manifest=augment_manifest_path,\n",
    "    translator_fn=import_fn,\n",
    ")\n",
    "print(f\"Augmentation manifest saved to {augment_manifest_path}\")\n",
    "\n",
    "print(f\"\\nContents:\")\n",
    "with open(augment_manifest_path, \"r\") as f:\n",
    "    print(f.readline()[:-1]) # (Strip trailing newline)\n",
    "print(\"...etc.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you wanted to import some other manifest file too:\n",
    "# util.smgt.translate_manifest_refs(\n",
    "#     source_manifest=\"data/[SUBFOLDER?]/manifests/output/output.manifest\", # Existing manifest file\n",
    "#     target_manifest=\"data/[SUBFOLDER?]/manifests/output/output.updated.manifest\", # New manifest file\n",
    "#     translator_fn=import_fn,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Merge the annotated datasets\n",
    "\n",
    "Now that our own labelling job is complete and we've imported other data to augment our data-set, we'll consolidate the batches together into a single combined manifest file.\n",
    "\n",
    "Since each annotation job may have had different names, and stored its labels to different fields in the output manifest, our merge will standardize the data to the `\"labels\"` field:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_manifest_data = util.smgt.merge_manifests(\n",
    "    \"labels\",\n",
    "    { \"file\": augment_manifest_path, \"field\": \"labels\" },\n",
    "    # If you couldn't finish your annotations, comment out the line below:\n",
    "    { \"file\": my_smgt_output_path, \"field\": my_groundtruth_labels },\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "print(f\"Got {len(merged_manifest_data)} total samples\")\n",
    "\n",
    "# The standardization above means these are always the attributes training will care about:\n",
    "attribute_names = [\"source-ref\", \"labels\"]\n",
    "%store attribute_names\n",
    "\n",
    "# For illustration, this is what an entry in our combined manifest looks like:\n",
    "print(f\"\\nMerged manifest contents:\")\n",
    "print(merged_manifest_data[0])\n",
    "print(\"...etc.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In object detection, we don't typically need to provide \"negative examples\" (images without the target objects), because the regions of each image that **aren't** the target object are already fulfilling that purpose.\n",
    "\n",
    "Some implementations of some algorithms may accept (and even benefit from) off-target images, but the GluonCV library we use in the final notebook doesn't get along with them - so we will cut them from our dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_manifest_data = list(filter(\n",
    "    lambda d: len(d[\"labels\"][\"annotations\"]) > 0,\n",
    "    merged_manifest_data\n",
    "))\n",
    "\n",
    "print(f\"Got {len(merged_manifest_data)} total samples after filtering out off-target images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Split training vs validation and upload final manifests\n",
    "\n",
    "Now we have all our consolidated label sets (and all the referenced images uploaded in our S3 bucket), the final step is to split training vs validation data and upload a manifest for each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split_index = round(len(merged_manifest_data)*0.8)\n",
    "train_data = merged_manifest_data[:train_test_split_index]\n",
    "validation_data = merged_manifest_data[train_test_split_index:]\n",
    "\n",
    "n_samples_training = len(train_data)\n",
    "%store n_samples_training\n",
    "n_samples_validation = len(validation_data)\n",
    "%store n_samples_validation\n",
    "\n",
    "with open(f\"{DATA_PREFIX}/train.manifest\", \"w\") as f:\n",
    "    for line in train_data:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "with open(f\"{DATA_PREFIX}/validation.manifest\", \"w\") as f:\n",
    "    for line in validation_data:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "bucket.upload_file(f\"{DATA_PREFIX}/train.manifest\", f\"{DATA_PREFIX}/train.manifest\")\n",
    "print(f\"Training manifest uploaded to:\\ns3://{BUCKET_NAME}/{DATA_PREFIX}/train.manifest\")\n",
    "bucket.upload_file(f\"{DATA_PREFIX}/validation.manifest\", f\"{DATA_PREFIX}/validation.manifest\")\n",
    "print(f\"Validation manifest uploaded to:\\ns3://{BUCKET_NAME}/{DATA_PREFIX}/validation.manifest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "Phew! That felt like a lot of work, but a lot of the steps were hacks for our example:\n",
    "\n",
    "* To find raw image data for our targets (boots and cats), we mapped our class names to the public OpenImages dataset and used their existing annotations to find relevant images.\n",
    "* To get a decent data volume without spending forever annotating in the workshop, we merged our Ground Truth annotation results with other augmentation sets.\n",
    "\n",
    "The useful points to remember are:\n",
    "\n",
    "* SageMaker Ground Truth (and as we'll see later, many of the built-in algorithms as well) uses **augmented manifests** to define annotated image datasets.\n",
    "* These manifests are just plain text [JSON Lines](http://jsonlines.org/) files that we can also edit in our own code to do whatever we like from importing/exporting annotations, to stitching together datasets as we did here.\n",
    "* Once the input manifest is prepared, it only takes a few clicks to define workforce teams and annotation jobs in SageMaker Ground Truth: Which supports other built-in and even custom annotation workflows for a variety of data types and tasks.\n",
    "\n",
    "Although we didn't use it here due to the dataset size, the [automated labelling](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-automated-labeling.html) feature can drastically cut annotation costs and time on bigger data-sets for the tasks where it's supported (including object detection).\n",
    "\n",
    "Ground Truth supports validation workflows (typically much faster for humans) as well as labelling; which can be combined with automated labelling in light of the importance of good quality ground truth input to effective machine learning.\n",
    "\n",
    "In the follow-on notebooks, we'll use the composite training and validation datasets we created here to fit a variety of models and compare their performance. Let's move on to notebook 2(a)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
