{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boots 'n' Cats 2c: Modelling with a Custom MXNet Algorithm\n",
    "\n",
    "In this notebook we'll try another approach to build our boots 'n' cats detector: a [GluonCV](https://gluon-cv.mxnet.io/)-based [YOLOv3](https://pjreddie.com/media/files/papers/YOLOv3.pdf) implementation on SageMaker's [MXNet container](https://sagemaker.readthedocs.io/en/stable/using_mxnet.html).\n",
    "\n",
    "SageMaker supports fully custom containers, but also offers pre-optimized environments for the major ML frameworks TensorFlow, PyTorch, and MXNet; which streamline typical workflows.\n",
    "\n",
    "The interface mechanisms (channels, endpoints, etc) work the same as for the built-in algorithms, but now we're authoring a Python package loaded by the framework application inside the base container: So need to understand the interfaces through which our code consumes inputs and exposes results and parameters.\n",
    "\n",
    "**You'll need to** have gone through the first notebook in this series (*Intro and Data Preparation*) to complete this example. The explanatory notes here also reference the equivalent steps of the previous notebook *SageMaker Built-In Algorithm* - so you'll want to read through that if you haven't already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Algorithm: YOLOv3\n",
    "\n",
    "As discussed with reference to benchmarks on the project [website](https://pjreddie.com/darknet/yolo/) and detailed in the [original paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf) (Redmon et al, 2016), YOLO (\"You Only Look Once\") is another highly successful object detection algorithm alongside SSD as implemented in the SageMaker built-in algorithm.\n",
    "\n",
    "Both YOLO and SSD are \"one-stage detectors\", in contrast to previous R-CNN group methods which separately 1) propose and then 2) validate and adjust bounding boxes. Tackling the region proposal and classification/validation problems together gives these architectures significant speed benefits at comparable accuracy.\n",
    "\n",
    "Whereas SSD creates convolutional \"feature maps\" at different scales and learns to predict the offset of \"anchor boxes\" relative to those; YOLO in parallel computes \"class probabilities\" on subdivided grid squares of the image, and bounding box coordinates for likely objects - before correlating the two together.\n",
    "\n",
    "A nice comparison of the methods is presented [here](https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html). Recent advances in YOLO (v2 and v3 releases) have led to it achieving better accuracy than SSD at comparable model sizes / speeds in some benchmarks, as shown in the below graph reproduced from the [GluonCV Model Zoo](https://gluon-cv.mxnet.io/model_zoo/detection.html)\n",
    "\n",
    "<img src=\"BlogImages/GluonCVYOLOvsSSD.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Dependencies and configuration\n",
    "\n",
    "As usual we'll start by loading libraries, defining configuration, and connecting to the AWS SDKs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# Built-Ins:\n",
    "import csv\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import imageio\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet as SageMakerMXNet\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Local Dependencies:\n",
    "%aimport util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we re-load configuration from the intro & data processing notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r BUCKET_NAME\n",
    "assert BUCKET_NAME, \"BUCKET_NAME missing from IPython store\"\n",
    "%store -r CHECKPOINTS_PREFIX\n",
    "assert CHECKPOINTS_PREFIX, \"CHECKPOINTS_PREFIX missing from IPython store\"\n",
    "%store -r DATA_PREFIX\n",
    "assert DATA_PREFIX, \"DATA_PREFIX missing from IPython store\"\n",
    "%store -r MODELS_PREFIX\n",
    "assert MODELS_PREFIX, \"MODELS_PREFIX missing from IPython store\"\n",
    "%store -r CLASS_NAMES\n",
    "assert CLASS_NAMES, \"CLASS_NAMES missing from IPython store\"\n",
    "%store -r test_image_folder\n",
    "assert test_image_folder, \"test_image_folder missing from IPython store\"\n",
    "\n",
    "%store -r attribute_names\n",
    "assert attribute_names, \"attribute_names missing from IPython store\"\n",
    "%store -r n_samples_training\n",
    "assert n_samples_training, \"n_samples_training missing from IPython store\"\n",
    "%store -r n_samples_validation\n",
    "assert n_samples_validation, \"n_samples_validation missing from IPython store\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just connect to the AWS SDKs we'll use, and validate the choice of S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "s3 = session.resource(\"s3\")\n",
    "bucket = s3.Bucket(BUCKET_NAME)\n",
    "smclient = session.client(\"sagemaker\")\n",
    "\n",
    "bucket_region = \\\n",
    "    session.client(\"s3\").head_bucket(Bucket=BUCKET_NAME) \\\n",
    "    [\"ResponseMetadata\"][\"HTTPHeaders\"][\"x-amz-bucket-region\"]\n",
    "assert (\n",
    "    bucket_region == region\n",
    "), f\"Your S3 bucket {BUCKET_NAME} and this notebook need to be in the same AWS region.\"\n",
    "\n",
    "# Initialise some empty variables we need to exist:\n",
    "predictor_std = None\n",
    "predictor_hpo = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Review our algorithm details\n",
    "\n",
    "We'll use the GluonCV (a deep learning framework built on MXNet) implementation of YOLOv3, and run it on top of the SageMaker-provided MXNet container.\n",
    "\n",
    "As detailed in the [SageMaker Python SDK docs](https://sagemaker.readthedocs.io/en/stable/using_mxnet.html), our job is to implement a Python file (or bundle of files with a designated entry point) that:\n",
    "\n",
    "* When run as a script, performs model training and saves the resultant model artifacts\n",
    "* When imported as a module, defines functions which the framework server application can call to load the model; perform inference; and deserialize/serialize data from and to the web.\n",
    "\n",
    "In cases like this one where extra dependencies (or newer versions) are required vs the base container, there are three options:\n",
    "\n",
    "* Define a custom container, either inheriting from the SageMaker base container; or creating a compatible fully-custom container\n",
    "* Supplying a `requirements.txt` file at the top level of the source bundle\n",
    "* Performing an inline `pip install` in the code itself, executed when the file is loaded\n",
    "\n",
    "In general the first option is preferred: pre-bundling dependencies into the container is more efficient for (potentially high-value, GPU-accelerated) compute resources than running an install script every time the training job starts or the new inference container instance starts up... But the other options can be useful for quick experimentation.\n",
    "\n",
    "Take some time to look through our implementation at the location below in this repository:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_point=\"yolo_train.py\"\n",
    "source_dir=\"src\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with built-in algorithms, the choices we make in implementation will have consequences including for example:\n",
    "\n",
    "* Whether distributed training is supported\n",
    "* Whether GPU-accelerated instances will provide any performance benefits\n",
    "* What data formats are supported for training and inference\n",
    "* How data is loaded into the container at training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set up input data channels\n",
    "\n",
    "From a script complexity perspective, the simplest approach would be to choose `s3_data_type=\"S3Prefix\"` and estimator `input_mode=\"File\"` (the defaults): in which case the matching files in S3 are simply downloaded to the container before the training job starts, and our training job code should load those to an in-memory dataset. This is the approach taken in the [official SageMaker MXNet MNIST example](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/mxnet_mnist).\n",
    "\n",
    "File mode is easy to use, but for jobs with large data sets we can cut container disk size and job startup time requirements by using **Pipe Mode** instead. Instead of downloading the full dataset to your container before starting work, Pipe Mode sets up a **stream** - which looks like a file on disk but only supports sequential access. Because the stream can't seek back to the start, SageMaker creates a new stream for each epoch - for example `/opt/ml/input/train/train_0` for the first.\n",
    "\n",
    "We'd still like SageMaker to shuffle our data, so how do we handle a use case like this where we need *both* the image file and its corresponding annotations to be useful?\n",
    "\n",
    "One solution could be to provide two separate channels for the images and the annotations (like the SageMaker built-in Image Classification algorithm when training with [\"Image Format\"](https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html)).\n",
    "\n",
    "A more feature-rich approach is the **AugmentedManifestFile** data type we used in the previous example. With this method, SageMaker steps through the manifest file and produces *multiple records in the stream for each row*: One for each (potentially filtered) attribute.\n",
    "\n",
    "Attribute names ending `-ref` (like our `source-ref`) are resolved to the raw contents of the S3 file URI they point to. Other attributes are passed through as plain text (i.e. JSON)... So in our case, the file stream in the container receives alternating records of image data and JSON annotations.\n",
    "\n",
    "As you might imagine (and will see in the training script file), using Pipe Mode and AugmentedManifestFile means we have to translate this stream into the (probably different) format our models expect. This is easier for some libraries than others (for e.g. there's a [PipeModeDataset](https://github.com/aws/sagemaker-tensorflow-extensions#sagemaker-pipe-mode) class built-in for TensorFlow users), and Gluon makes things particularly difficult by implementing random access as a standard interface of base dataset classes: making them unsuitable for streaming in general.\n",
    "\n",
    "Here we'll use the **same AugmentedManifestFile channel configuration as the built-in algorithm** from the previous notebook; and stream batches of data in to GluonCV datasets to train one after the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = sagemaker.session.s3_input(\n",
    "    f\"s3://{BUCKET_NAME}/{DATA_PREFIX}/train.manifest\",\n",
    "    distribution=\"FullyReplicated\",  # In case we want to try distributed training\n",
    "    content_type=\"application/x-recordio\",\n",
    "    s3_data_type=\"AugmentedManifestFile\",\n",
    "    record_wrapping=\"RecordIO\",\n",
    "    attribute_names=attribute_names,  # In case the manifest contains other junk to ignore (it does!)\n",
    "    shuffle_config=sagemaker.session.ShuffleConfig(seed=1337)\n",
    ")\n",
    "                                        \n",
    "validation_channel = sagemaker.session.s3_input(\n",
    "    f\"s3://{BUCKET_NAME}/{DATA_PREFIX}/validation.manifest\",\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"application/x-recordio\",\n",
    "    record_wrapping=\"RecordIO\",\n",
    "    s3_data_type=\"AugmentedManifestFile\",\n",
    "    attribute_names=attribute_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure the algorithm\n",
    "\n",
    "The core configuration process for the custom algorithm is the same as for the built-in algorithm (such as instance type/count, data source, execution role, and so on)... and we've written this script to accept training and inference data in the same format as the built-in.\n",
    "\n",
    "There are some differences too though, and some of the most significant ones are as follows:\n",
    "\n",
    "**Need to define metrics:** Metrics in SageMaker (like `validation:mAP` which we see in the SageMaker console and used to tune the built-in algorithm) are captured from the console output of the algorithm via a [regular expression](https://www.regular-expressions.info/). For the built-in algorithms, SageMaker already knows what metrics are defined... But for a custom algorithm (since we could be `print()`ing whatever we like), we need to supply the definitions.\n",
    "\n",
    "**No explicit \"training_image\"...:** As discussed in the last notebook, we only needed to use the generic `Estimator` SDK and supply a Docker image URI for the built-in SSD algorithm because there wasn't a specific SDK class for it. MXNet [has one](https://sagemaker.readthedocs.io/en/stable/using_mxnet.html) though, so the image URI is implicit. You can find more details about the (actually separate) SageMaker MXNet [training](https://github.com/aws/sagemaker-mxnet-container) and [inference](https://github.com/aws/sagemaker-mxnet-serving-container) containers in GitHub, as well as the shared [cross-framework base](https://github.com/aws/sagemaker-containers) they inherit from.\n",
    "\n",
    "**...But need to specify framework versions:** Because multiple incompatible versions of Python (2 vs 3), or the ML framework (e.g. TensorFlow 1 vs 2) may be supported at a given time.\n",
    "\n",
    "**Custom hyperparameters:** As with metrics, it's entirely up to us what hyperparameters we want to support and how: So any hyperparameters are fully custom to our code in the [src](src) folder. There's also a little inconsistency between frameworks and built-in algorithms about whether hyperparameters are passed in as a `hyperparameters` argument in the constructor or with a separate `set_hyperparameters()` function, but otherwise the process is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    # You should see these metrics appear in the SageMaker console on the details page for the\n",
    "    # training job when it starts (and see them in the log stream during `.fit()`):\n",
    "    { \"Name\": \"train:TrainingCost\", \"Regex\": r\"\\[Epoch \\d+\\] .*TrainingCost=(.*?);\" },\n",
    "    { \"Name\": \"train:ObjLoss\", \"Regex\": r\"\\[Epoch \\d+\\] .*ObjLoss=(.*?);\" },\n",
    "    { \"Name\": \"train:BoxCenterLoss\", \"Regex\": r\"\\[Epoch \\d+\\] .*BoxCenterLoss=(.*?);\" },\n",
    "    { \"Name\": \"train:BoxScaleLoss\", \"Regex\": r\"\\[Epoch \\d+\\] .*BoxScaleLoss=(.*?);\" },\n",
    "    { \"Name\": \"train:ClassLoss\", \"Regex\": r\"\\[Epoch \\d+\\] .*ClassLoss=(.*?);\" },\n",
    "    { \"Name\": \"train:TotalLoss\", \"Regex\": r\"\\[Epoch \\d+\\] .*TotalLoss=(.*?);\" },\n",
    "    { \"Name\": \"train:MeanAP\", \"Regex\": r\"Train: VOCMeanAP=(.*?);\" },\n",
    "    { \"Name\": \"validation:MeanAP\", \"Regex\": r\"Validation: VOCMeanAP=(.*?);\" },\n",
    "]\n",
    "\n",
    "# Review which of these parameters correspond to the built-in notebook, and which are new:\n",
    "estimator = SageMakerMXNet(\n",
    "    role=role,\n",
    "    entry_point=entry_point,\n",
    "    source_dir=source_dir,\n",
    "    framework_version=\"1.4.1\",\n",
    "    py_version=\"py3\",\n",
    "    input_mode=\"Pipe\",\n",
    "    train_instance_count=1,  # Note: Our implementation doesn't actually support multi-instance yet\n",
    "    train_instance_type=\"ml.p3.2xlarge\",\n",
    "    train_volume_size=50,\n",
    "    train_max_run=60*60,\n",
    "    train_use_spot_instances=True,\n",
    "    train_max_wait=60*60,\n",
    "    metric_definitions=metric_definitions,\n",
    "    base_job_name=\"bootsncats-yolo\",\n",
    "    output_path=f\"s3://{BUCKET_NAME}/{MODELS_PREFIX}\",\n",
    "    checkpoint_s3_uri=f\"s3://{BUCKET_NAME}/{CHECKPOINTS_PREFIX}\",\n",
    "    # There's more documentation of the hyperparameters in the script itself:\n",
    "    hyperparameters={\n",
    "        \"epochs\": 200,\n",
    "        \"learning-rate\": 0.0001,\n",
    "        \"num-classes\": 2,\n",
    "        \"num-workers\": 0,  # Current script has a bug around multithreading data access\n",
    "        \"num-samples-train\": n_samples_training,  # Needed for GluonCV\n",
    "        \"num-samples-validation\": n_samples_validation,  # Needed for GluonCV\n",
    "        \"batch-size\": 4,\n",
    "        \"data-shape\": 416,\n",
    "        \"random-seed\": 1337,\n",
    "        \"log-interval\": 1,  # Log intermediate metrics every minibatch\n",
    "        \"early-stopping\": \"yes\",\n",
    "        #\"log-level\": \"DEBUG\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train the model\n",
    "\n",
    "As with the built-in algorithms, we have the choice between fitting our model with the given set of hyperparameters or performing automatic hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WITH_HPO = False# TODO: True first, then False?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if (not WITH_HPO):\n",
    "    estimator.fit(\n",
    "        {\n",
    "            \"train\": train_channel,\n",
    "            \"validation\": validation_channel\n",
    "        },\n",
    "        logs=True\n",
    "    )\n",
    "else:\n",
    "    hyperparameter_ranges = {\n",
    "        \"learning-rate\": sagemaker.tuner.ContinuousParameter(0.00001, 0.01),\n",
    "        \"momentum\": sagemaker.tuner.ContinuousParameter(0.0, 0.99),\n",
    "        \"weight_decay\": sagemaker.tuner.ContinuousParameter(0.0, 0.99),\n",
    "        \"batch-size\": sagemaker.tuner.IntegerParameter(1, 4),\n",
    "        \"optimizer\": sagemaker.tuner.CategoricalParameter([\"sgd\", \"adam\", \"rmsprop\", \"adadelta\"])\n",
    "    }\n",
    "\n",
    "    tuner = sagemaker.tuner.HyperparameterTuner(\n",
    "        estimator,\n",
    "        \"validation:MeanAP\",  # Name of the objective metric to optimize\n",
    "        objective_type=\"Maximize\",  # \"Mean Average Precision\" high = good\n",
    "        metric_definitions=metric_definitions,\n",
    "        hyperparameter_ranges=hyperparameter_ranges,\n",
    "        base_tuning_job_name=\"bootsncats-yolo-hpo\",\n",
    "        # `max_jobs` obviously has cost implications, but the optimization can always be terminated:\n",
    "        max_jobs=24,\n",
    "        max_parallel_jobs=3  # Keep sensible for the configured max_jobs...\n",
    "    )\n",
    "    \n",
    "    tuner.fit(\n",
    "        {\n",
    "            \"train\": train_channel,\n",
    "            \"validation\": validation_channel\n",
    "        },\n",
    "        include_cls_metadata=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, if we ever lose notebook state e.g. due to a kernel restart or crash, we can `attach()` our estimator/tuner to a previous training/tuning job as follows: (No need to re-train - the results are all stored!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples to attach to a previous training run:\n",
    "#estimator.attach(\"bootsncats-yolo-hpo-191209-1637-003-bbdca4b2\")\n",
    "#tuner.attach(\"bootsncats-yolo-hpo-191209-1637\")\n",
    "#WITH_HPO=?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: While the model(s) are training\n",
    "\n",
    "Remember to go back to the previous notebooks if you still have steps unfinished...\n",
    "\n",
    "Otherwise, take some time to read through and understand the code in the src folder! This implementation is made unusually complex, because of the nature of object detection data (image + bounding boxes) and our decision to support the same I/O as the built-in algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Deploy the model\n",
    "\n",
    "Deploying our trained custom model is as simple as for the built-in algorithm, with just one extra step:\n",
    "\n",
    "We need to warn the `predictor` our request body will be an image, so it passes this information along to our container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if (WITH_HPO):\n",
    "    if (predictor_hpo):\n",
    "        try:\n",
    "            predictor_hpo.delete_endpoint()\n",
    "            print(\"Deleted previous HPO endpoint\")\n",
    "        except:\n",
    "            print(\"Couldn't delete previous HPO endpoint\")\n",
    "    print(\"Deploying HPO model...\")\n",
    "    predictor_hpo = tuner.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=\"ml.m5.large\",\n",
    "        # wait=False,\n",
    "    )\n",
    "    predictor_hpo.content_type = \"application/x-image\"\n",
    "else:\n",
    "    if (predictor_std):\n",
    "        try:\n",
    "            predictor_std.delete_endpoint()\n",
    "            print(\"Deleted previous non-HPO endpoint\")\n",
    "        except:\n",
    "            print(\"Couldn't delete previous non-HPO endpoint\")\n",
    "    print(\"Deploying standard (non-HPO) model...\")\n",
    "    predictor_std = estimator.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=\"ml.m5.large\",\n",
    "        # wait=False,\n",
    "    )\n",
    "    predictor_std.content_type = \"application/x-image\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Run inference on test images\n",
    "\n",
    "We've set our custom algorithm up like the built-in, to return *all* detections and let the caller filter by whatever confidence threshold seems to perform best.\n",
    "\n",
    "Training this algorithm can be even trickier than SSD though: are you able to get good results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change this if you want something different:\n",
    "predictor = predictor_hpo if WITH_HPO else predictor_std\n",
    "\n",
    "# This time confidence is 0-1, not 0-100:\n",
    "confidence_threshold = # TODO: 0.2 is a good starting point, but explore options!\n",
    "\n",
    "for test_image in os.listdir(test_image_folder):\n",
    "    test_image_path = f\"{test_image_folder}/{test_image}\"\n",
    "    with open(test_image_path, \"rb\") as f:\n",
    "        payload = bytearray(f.read())\n",
    "\n",
    "    client = boto3.client(\"sagemaker-runtime\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=predictor.endpoint,\n",
    "        ContentType='application/x-image',\n",
    "        Body=payload\n",
    "    )\n",
    "\n",
    "    result = response['Body'].read()\n",
    "    result = json.loads(result)[\"prediction\"]\n",
    "    print(result)\n",
    "    # result is a list of [class_ix, confidence, x1, y1, x2, y2] detections.\n",
    "    # (x/y locations are relative/normalized)\n",
    "    display(HTML(f\"<h4>{test_image}</h4>\"))\n",
    "    util.visualize_detection(\n",
    "        test_image_path,\n",
    "        result,\n",
    "        CLASS_NAMES,\n",
    "        thresh=confidence_threshold\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Although training instances are ephemeral, the resources we allocated for real-time endpoints need to be cleaned up to avoid ongoing charges.\n",
    "\n",
    "The code below will delete the *most recently deployed* endpoint for the HPO and non-HPO configurations, but note that if you deployed either more than once, you might end up with extra endpoints.\n",
    "\n",
    "To be safe, it's best to still check through the SageMaker console for any left-over resources when cleaning up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (predictor_hpo):\n",
    "    print(\"Deleting HPO-optimized predictor endpoint\")\n",
    "    predictor_hpo.delete_endpoint()\n",
    "if (predictor_std):\n",
    "    print(\"Deleting standard (non-HPO) predictor endpoint\")\n",
    "    predictor_std.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review TODO\n",
    "\n",
    "**TODO: Review**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
